<!DOCTYPE html>
<html>
<head>
<title>Australia considers ban on ‘high-risk’ uses of AI such as deepfakes and algorithmic bias</title>
<meta charset="UTF-8">
    <style>
        body {
            font-family: Arial, sans-serif;
        }
    </style>
    </head>
<body>
<h1>Australia considers ban on ‘high-risk’ uses of AI such as deepfakes and algorithmic bias</h1>
<p>Publication Date: 2023-05-31</p>
<p>Author: Paul Karp</p>
<p>Section: Technology</p>
<p>Tags: Artificial intelligence (AI), Australian politics, Computing, news</p>
<p>Article URL: <a href='https://www.theguardian.com/technology/2023/jun/01/australian-government-considers-ban-on-high-risk-uses-of-ai-such-as-deepfakes-and-algorithmic-bias' target='_blank'>https://www.theguardian.com/technology/2023/jun/01/australian-government-considers-ban-on-high-risk-uses-of-ai-such-as-deepfakes-and-algorithmic-bias</a></p>
<img src='https://media.guim.co.uk/835cfc0bde2fbb7ba13696027564709566a44956/0_193_4608_2765/500.jpg' alt='Article Image'>
<p>The Albanese government is considering a ban on “high-risk” uses of artificial intelligence and automated decision-making, warning of potential harms including the creation of deepfakes and algorithmic bias. On Thursday, the industry and science minister, Ed Husic, will release a report on the emerging technologies by the National Science and Technology Council and a discussion paper on how to achieve “safe and responsible” AI. Generative AI, in which AI creates new content such as text, images, audio and code, has experienced a surge in uptake such as through the “large language model” programs ChatGPT, Google’s chatbot Bard and Microsoft Bing’s chat feature. While universities and education authorities grapple with the new technology’s application in student cheating, the industry department’s discussion paper warns AI has a range of “potentially harmful purposes”. These include “generating deepfakes to influence democratic processes or cause other deceit, creating misinformation and disinformation, [and] encouraging people to self-harm”. “Algorithmic bias is often raised as one of the biggest risks or dangers of AI,” it said, with the potential to prioritise male over female candidates in recruitment or to target minority racial groups. The paper also noted positive applications of AI already in use such as analysing medical images, improving building safety and cost savings in provision of legal services. The implications of AI on the labour market, national security and intellectual property were outside its scope. The NSTC report found that “the concentration of generative AI resources within a small number of large multinational and primarily US-based technology companies poses potentials risks to Australia”. While Australia has some advantages in computer vision and robotics, its “core fundamental capacity in [large language models] and related areas is relatively weak” due to “high barriers to access”. The paper sets out a range of responses from around the world: from voluntary approaches in Singapore to greater regulation in the EU and Canada. “There is a developing international direction towards a risk-based approach for governance of AI,” it said. The paper said the government will “ensure there are appropriate safeguards, especially for high-risk applications of AI and [automated decision-making]”. In a snap eight-week consultation, the paper asked stakeholders “whether any high-risk AI applications or technologies should be banned completely” and, if so, what criteria should be applied for banning them. But the paper noted that Australia may need to harmonise its governance with major trading partners in order to take “advantage of AI-enabled systems supplied on a global scale and foster the growth of AI in Australia”. The paper asks stakeholders to consider “the implications for Australia’s domestic tech sector and our current trading and export activities with other countries if we took a more rigorous approach to ban certain high-risk activities”. Husic said “using AI safely and responsibly is a balancing act the whole world is grappling with at the moment”. “The upside is massive, whether it’s fighting superbugs with new AI-developed antibiotics or preventing online fraud,” he said in a statement. “But as I have been saying for many years, there needs to be appropriate safeguards to ensure the safe and responsible use of AI. “Today is about what we do next to build trust and public confidence in these critical technologies.” Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup In the budget the federal government invested $41m for the National AI Centre, which sits within the science agency CSIRO, and a new Responsible AI Adopt program for small and medium enterprises. The paper noted that, since Australia’s laws are “technology neutral”, AI is already regulated to an extent by existing laws including on consumer protection, online safety, privacy and criminal laws. For example, the hotel booking website Trivago has paid penalties for algorithmic decision-making that misled consumers into thinking they were offered the cheapest rates. In April a regional Australian mayor said he may sue OpenAI if it does not correct ChatGPT’s false claims that he had served time in prison for bribery, in what would be the first defamation lawsuit against the automated text service. In May the eSafety commissioner warned that generative AI programs could be used to automate child grooming by predators. The Labor MP Julian Hill, who warned about uncontrollable military applications of AI in parliament in February, has called for a new Australian AI Commission to regulate AI.</p>
</body>
</html>